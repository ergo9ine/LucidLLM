# LucidLLM Chat

[한국어](README.ko.md) | [English](../README.md) | [日本語](README.ja.md) | [简体中文](README.zh-CN.md)

![License](https://img.shields.io/github/license/ergo9ine/LucidLLM)
![Transformers.js](https://img.shields.io/badge/Transformers.js-v4.0.0-yellow)
![WebGPU](https://img.shields.io/badge/WebGPU-Supported-green)
![PWA](https://img.shields.io/badge/PWA-Planned-blue)

**LucidLLM**は、[Transformers.js](https://huggingface.co/docs/transformers.js)とWebGPU技術を使用して、AIモデルをブラウザ内で完全にローカルで実行するチャットアプリケーションです。ビルド不要（Zero-build）のアーキテクチャと完全なプライバシー保護を提供し、外部サーバーにデータを送信することなく強力なAI機能を実現します。

> **主な特徴:** 18,200行以上のコード • 4ヶ国語対応 • WebGPU/WASM推論アクセラレーション • リアルタイムトークンストリーミング • カスタム仮想DOM • AES-256暗号化バックアップ • OPFSモデルキャッシュ

## ✨ 主な機能

### 🤖 AIとモデル

| 機能 | 説明 |
|------|------|
| **完全ローカル推論** | すべてのAI推論がブラウザ内で実行され、データがデバイスから出ることはありません。 |
| **WebGPUアクセラレーション** | WebGPUによる高速推論をサポートし、非対応ブラウザでは自動的にWASMにフォールバックします。 |
| **OPFSモデルキャッシュ** | Origin Private File Systemを使用してモデルを永続的に保存し、再ダウンロードを防ぎます。 |
| **スマートダウンロード** | ダウンロードの一時停止/再開、指数バックオフ（Exponential Backoff）リトライ、量子化（Quantization）選択をサポートします。 |
| **マルチモデル対応** | 複数のONNXモデルをキャッシュし、自由に切り替えて使用できます。 |
| **モデルカード表示** | モデルのメタデータ（アップローダー、タスク、ライセンス、いいね数など）を詳細に表示します。 |
| **Hugging Face連携** | Hugging Faceハブから直接モデルを検索してダウンロードできます。 |

### 💬 チャット体験

| 機能 | 説明 |
|------|------|
| **マルチセッションチャット** | 複数の独立したチャットタブを作成し、会話履歴を個別に管理できます。 |
| **リアルタイムストリーミング** | トークン生成プロセスをリアルタイムでストリーミング表示します。 |
| **トークン速度統計** | 1秒あたりのトークン数（TPS）統計（平均/最大/最小）をリアルタイムで表示します。 |
| **メモリ使用量表示** | リアルタイムでメモリ消費量を監視します。 |
| **生成中断** | いつでもワンクリックでAIの応答生成を即座に中断できます。 |
| **システムプロンプト** | AIアシスタントの役割や振る舞いを定義するシステムプロンプトをカスタマイズできます。 |
| **コンテキスト制御** | 4kから128kまでコンテキストウィンドウのサイズを調整できます。 |
| **チャットのエクスポート** | 会話内容をJSONファイルとしてバックアップおよびエクスポートできます。 |
| **自動スクロール** | 生成中に自動で下部にスクロールし、手動オーバーライドボタンも提供します。 |

### 🔒 プライバシーとバックアップ

| 機能 | 説明 |
|------|------|
| **Googleドライブバックアップ** | 設定とチャット履歴をGoogleドライブに安全にバックアップします。 |
| **強力な暗号化** | PBKDF2鍵導出（250,000回反復）とAES-GCM-256アルゴリズムを使用してクライアント側で暗号化します。 |
| **Gzip圧縮** | バックアップペイロードをオプションでGzip圧縮します。 |
| **自動バックアップ** | データ変更時に自動的にバックアップを実行します（デバウンス25秒適用）。 |
| **バックアップ復元** | 特定の時点のバックアップスナップショットから復元できます。 |
| **バージョン管理** | 複数のバックアップバージョンを保持し、任意の時点に復元できます。 |
| **サーバー通信なし** | 明示的なバックアップ以外、データが外部サーバーに送信されることはありません。 |

### 🌐 ユーザー体験

| 機能 | 説明 |
|------|------|
| **4ヶ国語対応** | 韓国語、英語、日本語、中国語（簡体字）に対応し、ブラウザの言語を自動検出します。 |
| **4つのテーマ** | ダークモード、ライトモード、OLEDブラックモード（純粋な黒）、ハイコントラストモードをサポートしています。 |
| **レスポンシブデザイン** | モバイル、タブレット、デスクトップなど、あらゆるデバイスに最適化されたUIを提供します。 |
| **PWAサポート** | プログレッシブウェブアプリ機能（予定） |
| **サイドバーナビゲーション** | チャットとワークスペースパネルを備えた折りたたみ可能なサイドバーを提供します。 |
| **ショートカットキー** | Ctrl+N（新規チャット）、Ctrl+Enter（送信）、Ctrl+L（入力フォーカス）、Ctrl+,（設定）、Ctrl+Shift+Backspace（削除）、Ctrl+Shift+E（エクスポート）、Ctrl+B（サイドバー）、Ctrl+/（ヘルプ） |

## 📋 システム要件

### ブラウザ互換性

| 要件 | 詳細 |
|------|------|
| **推奨ブラウザ** | Chrome 113+ / Edge 113+ (WebGPU対応) |
| **最小要件** | WASMをサポートするすべての最新ブラウザ |
| **セキュリティコンテキスト** | OPFS使用のため、HTTPSまたはlocalhost環境が必須 |
| **JavaScript** | ES2020+モジュールサポート |

### ハードウェア推奨スペック

| 項目 | 最小スペック | 推奨スペック |
|------|--------------|--------------|
| **RAM** | 4GB | 8GB以上 |
| **GPU** | 内蔵グラフィックス | WebGPU対応の外付けグラフィックス |
| **ストレージ** | モデルあたり100MB〜2GB | SSD推奨 |

### 推奨モデル

| モデル | サイズ | 量子化 | 用途 |
|--------|--------|--------|------|
| SmolLM2-135M-Instruct | ~135M | FP32, BNB4 | テスト/開発 |
| Qwen2.5-0.5B-Instruct | ~500M | Q4 | バランス型 |
| Phi-4-mini-instruct | ~3.8B | Q4 | 高品質な応答 |

## 🚀 クイックスタート

### ライブデモ

GitHub Pagesのホスト版をすぐに試せます（インストール不要）：

👉 **https://ergo9ine.github.io/LucidLLM/**

### ローカル（ゼロビルド）

```bash
git clone https://github.com/ergo9ine/LucidLLM.git
cd LucidLLM
npm run serve    # http://localhost:3000 で起動
```

(代替: `python -m http.server 8000` または `npx serve .`)

ブラウザで開き、Settings → Model Management からモデルをダウンロードして有効化してください。

### 開発とテスト

- オプション: `npm install`（テスト/開発ツールの実行時のみ必要）
- ユニットテスト: `npm test`（Vitest）
- E2Eテスト: `npx playwright test`

---

## 📖 使用ガイド

### 1. モデルの読み込み

1. ヘッダーの **設定(⚙️)** ボタンをクリックします。
2. **モデル管理** タブに移動します。
3. Hugging FaceモデルIDを入力します（例：`onnx-community/SmolLM2-135M-Instruct`）。
4. **検索** ボタンを押してモデル情報を取得します。
5. 希望する **量子化(Quantization)** オプションを選択します。
6. **ダウンロード** をクリックしてモデルをOPFSにキャッシュします。（レジューム対応）
7. ダウンロードが完了したら **有効化** ボタンを押してモデルをロードします。

### 2. チャットを開始する

1. 下部の入力欄にメッセージを入力し、**送信** ボタンを押すか `Ctrl+Enter` キーを押します。
2. タブバーの **+** ボタンで新しいチャットセッションを作成できます。
3. タブをクリックして複数の会話セッションを自由に切り替えることができます。
4. **中断** ボタン（または `Ctrl+Shift+Backspace`）でいつでも生成を中断できます。

### 3. LLM設定

**設定 > LLM設定** で以下の項目を調整できます：

| 設定 | デフォルト値 | 説明 |
|------|--------------|------|
| **システムプロンプト** | "You are a helpful assistant." | AIの役割と性格を定義します。 |
| **最大出力トークン** | 512 | 1回の応答で生成する最大長を制限します。 |
| **コンテキストウィンドウ** | 8k | モデルが記憶できる会話の長さを設定します。 |
| **温度 (Temperature)** | 0.9 | 応答の創造性とランダム性を調整します。 |

### 4. Googleドライブバックアップ

1. **設定 > バックアップと復元** に移動します。
2. **Googleドライブに接続** ボタンをクリックしてログインします。（クライアントIDは自動設定されます）
3. **自動バックアップ** を有効にすると、変更があるたびに自動的に保存されます（25秒デバウンス）。
4. **今すぐバックアップ** ボタンで手動バックアップも可能です。
5. 復元リストから希望する時点を選択し、**復元** ボタンを押すと以前の状態に戻せます。

---

## 🛠️ 開発ガイド

- ランタイム: バニラ ES モジュール（バンドラ不要）
- 主要ファイル:
  - `script/bootstrap.js` — 起動処理とハイドレーション
  - `script/main.js` — UI 状態・ハンドラおよびレンダリング
  - `script/i18n.js` — 多言語処理モジュール（韓国語、英語、日本語、中国語簡体字）
  - `script/worker.js` — 推論ワーカーとパイプライン管理
  - `script/shared-utils.js` — 共通ユーティリティ
  - `script/drive-backup.js` — 暗号化バックアップ
- テスト: Vitest ユニットテスト (`npm test`)、Playwright E2Eテスト (`npx playwright test`)
- デバッグ: DevToolsで `state` コンソール確認、`opfs` マニフェストと `transformers` パイプラインキャッシュを確認

---

## 🤝 貢献について

- 大きな変更は事前に Issue で相談してください。
- PR フロー: fork → branch → PR（説明・スクリーンショット・テストを添えて）

---

## 🔒 セキュリティとプライバシー

- 推論とチャットデータはデフォルトでローカルに留まります。
- Google Drive へのバックアップは任意で、クライアント側で暗号化されます。
- 機微なモデルやデータを公開場所へアップロードしないでください。

## 🏗️ プロジェクト構造

```
LucidLLM/
├── index.html                  # メインHTMLエントリーポイント
├── script/
│   ├── bootstrap.js            # アプリケーション初期化
│   ├── main.js                 # コアロジックおよび状態管理
│   ├── i18n.js                 # 多言語処理モジュール
│   ├── shared-utils.js         # 共通ユーティリティ
│   ├── worker.js               # 推論用Web Worker
│   └── drive-backup.js         # Googleドライブバックアップロジック
├── docs/                       # ドキュメントと多言語README
├── favicon.svg                 # アプリアイコン
└── package.json                # NPM設定
```

## 🛠️ 技術スタック

| カテゴリ | 技術 |
|----------|------|
| **言語** | JavaScript (ES2020+ Modules) |
| **アーキテクチャ** | Zero-build, Vanilla JS (No Framework) |
| **MLフレームワーク** | Transformers.js v4.0.0 |
| **推論バックエンド** | WebGPU / WASM (自動切り替え) |
| **ストレージ** | Origin Private File System (OPFS), localStorage |
| **スタイリング** | Tailwind CSS v3 (CDN) + Custom CSS Variables |
| **アイコン** | Lucide Icons (CDN) |
| **フォント** | Space Grotesk (Google Fonts) |
| **認証** | Google Identity Services (OAuth 2.0) |
| **暗号化** | Web Crypto API (PBKDF2, AES-GCM-256) |
| **CDN** | jsDelivr, unpkg |
| **テスト** | Vitest（ユニット）、Playwright（E2E） |

## 📄 ライセンス

このプロジェクトは **MITライセンス** の下で配布されています。詳細は [LICENSE](../LICENSE) ファイルを参照してください。

## 🙏 謝辞

- [Hugging Face](https://huggingface.co/) - Transformers.jsおよびモデルホスティング
- [Transformers.js](https://huggingface.co/docs/transformers.js) - ブラウザ内ML推論ライブラリ
- [Tailwind CSS](https://tailwindcss.com/) - ユーティリティファーストCSSフレームワーク
- [Lucide Icons](https://lucide.dev/) - 美しいオープンソースアイコン
- [Space Grotesk](https://fonts.google.com/specimen/Space+Grotesk) - フォントファミリー

---

**Made with ❤️ for privacy-focused AI**

[⬆トップに戻る](#lucidllm-chat)
